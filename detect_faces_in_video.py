#!/usr/bin/env python3
"""
åŸºäº FaceFusion çš„è§†é¢‘äººè„¸æ£€æµ‹è„šæœ¬
æ£€æµ‹è§†é¢‘ä¸­çš„æ‰€æœ‰äººè„¸ï¼Œæ”¯æŒå¤šäººè„¸è·Ÿè¸ªå’Œç»Ÿè®¡
"""

import os
import sys
import json
import argparse
from typing import List, Dict, Any, Optional
from pathlib import Path
from collections import defaultdict
import numpy as np

# è®¾ç½® FaceFusion è·¯å¾„
FACEFUSION_ROOT = os.environ.get('FACEFUSION_ROOT', '/path/to/facefusion')
if os.path.exists(FACEFUSION_ROOT):
    sys.path.insert(0, FACEFUSION_ROOT)

try:
    import facefusion.state_manager as state_manager
    from facefusion.face_analyser import get_many_faces
    from facefusion.vision import read_video_frame, count_video_frame_total, detect_video_fps, detect_video_resolution
    from facefusion.face_helper import calculate_face_distance
    from facefusion.face_store import clear_static_faces
    print("âœ… FaceFusion åŠ è½½æˆåŠŸ")
    FACEFUSION_AVAILABLE = True
except ImportError as e:
    print(f"âŒ FaceFusion ä¸å¯ç”¨: {e}")
    print("è¯·è®¾ç½®: export FACEFUSION_ROOT=/path/to/facefusion")
    FACEFUSION_AVAILABLE = False
    sys.exit(1)


def detect_video_faces(video_path: str,
                       model: str = 'yolo_face',
                       threshold: float = 0.5,
                       mode: str = 'many',
                       sample_interval: int = 1,
                       start_frame: int = 0,
                       end_frame: Optional[int] = None) -> Dict[str, Any]:
    """
    æ£€æµ‹è§†é¢‘ä¸­çš„æ‰€æœ‰äººè„¸
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        model: æ£€æµ‹æ¨¡å‹ (yolo_face, retinaface, scrfd, yunet, many)
        threshold: æ£€æµ‹é˜ˆå€¼
        mode: äººè„¸é€‰æ‹©æ¨¡å¼ (one, many, reference)
        sample_interval: é‡‡æ ·é—´éš”
        start_frame: èµ·å§‹å¸§
        end_frame: ç»“æŸå¸§
    
    Returns:
        æ£€æµ‹ç»“æœå­—å…¸
    """
    # é…ç½® FaceFusion
    state_manager.set_item('face_detector_model', model)
    state_manager.set_item('face_detector_size', '640x640')
    state_manager.set_item('face_detector_score', threshold)
    state_manager.set_item('face_detector_angles', [0])
    state_manager.set_item('face_selector_mode', mode)
    state_manager.set_item('face_landmarker_model', '2dfan4')
    state_manager.set_item('face_recognizer_model', 'arcface_inswapper_128')
    
    # æ¸…ç©ºç¼“å­˜
    clear_static_faces()
    
    # è·å–è§†é¢‘ä¿¡æ¯
    total_frames = count_video_frame_total(video_path)
    fps = detect_video_fps(video_path)
    width, height = detect_video_resolution(video_path)
    
    if end_frame is None:
        end_frame = total_frames
    else:
        end_frame = min(end_frame, total_frames)
    
    print(f"\nğŸ“¹ è§†é¢‘ä¿¡æ¯:")
    print(f"  åˆ†è¾¨ç‡: {width}x{height}")
    print(f"  å¸§ç‡: {fps:.2f} FPS")
    print(f"  æ€»å¸§æ•°: {total_frames}")
    print(f"  æ£€æµ‹èŒƒå›´: å¸§ {start_frame}-{end_frame}, é—´éš” {sample_interval}")
    
    # æ£€æµ‹äººè„¸
    all_faces = []
    frame_faces = defaultdict(list)
    unique_persons = {}
    person_id_counter = 1
    
    print(f"\nğŸ” å¼€å§‹æ£€æµ‹...")
    for frame_idx in range(start_frame, end_frame, sample_interval):
        # è¯»å–å¸§
        frame = read_video_frame(video_path, frame_idx)
        if frame is None:
            continue
        
        # æ£€æµ‹äººè„¸
        faces = get_many_faces([frame])
        
        # å¤„ç†æ¯ä¸ªäººè„¸
        for face_idx, face in enumerate(faces):
            # æå–ä¿¡æ¯
            face_info = {
                'frame': frame_idx,
                'index': face_idx,
                'bbox': face.bbox.tolist() if hasattr(face, 'bbox') else None,
                'score': float(face.score) if hasattr(face, 'score') else 1.0,
                'gender': face.gender if hasattr(face, 'gender') else None,
                'age': int(face.age) if hasattr(face, 'age') and face.age else None,
                'race': face.race if hasattr(face, 'race') else None
            }
            
            # äººè„¸è·Ÿè¸ªï¼ˆåŸºäº embeddingï¼‰
            if hasattr(face, 'embedding_norm') and face.embedding_norm is not None:
                person_id = None
                min_distance = float('inf')
                
                # æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„å·²çŸ¥äººè„¸
                for pid, person_data in unique_persons.items():
                    distance = calculate_face_distance(face, person_data['reference_face'])
                    if distance < 0.4 and distance < min_distance:  # é˜ˆå€¼ 0.4 (ç›¸ä¼¼åº¦ > 0.6)
                        person_id = pid
                        min_distance = distance
                
                # å¦‚æœæ²¡æ‰¾åˆ°åŒ¹é…ï¼Œåˆ›å»ºæ–°äºº
                if person_id is None:
                    person_id = f"person_{person_id_counter}"
                    person_id_counter += 1
                    unique_persons[person_id] = {
                        'reference_face': face,
                        'first_frame': frame_idx,
                        'last_frame': frame_idx,
                        'appearances': [],
                        'gender': face_info['gender'],
                        'age': face_info['age'],
                        'race': face_info['race']
                    }
                else:
                    # æ›´æ–°æœ€åå‡ºç°å¸§
                    unique_persons[person_id]['last_frame'] = frame_idx
                
                face_info['person_id'] = person_id
                unique_persons[person_id]['appearances'].append(frame_idx)
            
            all_faces.append(face_info)
            frame_faces[frame_idx].append(face_info)
        
        # è¿›åº¦æ˜¾ç¤º
        if frame_idx % 30 == 0:
            progress = (frame_idx - start_frame) / (end_frame - start_frame) * 100
            print(f"  è¿›åº¦: {progress:.1f}%")
    
    # ç»Ÿè®¡ç»“æœ
    result = {
        'video_path': video_path,
        'video_info': {
            'width': width,
            'height': height,
            'fps': fps,
            'total_frames': total_frames
        },
        'detection_settings': {
            'model': model,
            'threshold': threshold,
            'mode': mode,
            'sample_interval': sample_interval,
            'frames_processed': (end_frame - start_frame) // sample_interval
        },
        'statistics': {
            'total_detections': len(all_faces),
            'unique_persons': len(unique_persons),
            'frames_with_faces': len(frame_faces),
            'max_faces_per_frame': max(len(faces) for faces in frame_faces.values()) if frame_faces else 0
        },
        'persons': {},
        'frame_data': {}
    }
    
    # æ•´ç†äººå‘˜ä¿¡æ¯
    for person_id, person_data in unique_persons.items():
        result['persons'][person_id] = {
            'gender': person_data['gender'],
            'age': person_data['age'],
            'race': person_data['race'],
            'first_frame': person_data['first_frame'],
            'last_frame': person_data['last_frame'],
            'total_appearances': len(person_data['appearances'])
        }
    
    # æ•´ç†å¸§æ•°æ®ï¼ˆç²¾ç®€ç‰ˆï¼‰
    for frame_idx, faces in frame_faces.items():
        result['frame_data'][str(frame_idx)] = {
            'face_count': len(faces),
            'person_ids': [f.get('person_id', f"unknown_{f['index']}") for f in faces]
        }
    
    return result


def main():
    parser = argparse.ArgumentParser(description='æ£€æµ‹è§†é¢‘ä¸­çš„äººè„¸')
    parser.add_argument('video', help='è§†é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model', default='yolo_face', 
                       choices=['yolo_face', 'retinaface', 'scrfd', 'yunet', 'many'],
                       help='æ£€æµ‹æ¨¡å‹')
    parser.add_argument('--threshold', type=float, default=0.5, help='æ£€æµ‹é˜ˆå€¼')
    parser.add_argument('--mode', default='many',
                       choices=['one', 'many', 'reference'],
                       help='äººè„¸é€‰æ‹©æ¨¡å¼')
    parser.add_argument('--interval', type=int, default=5, help='é‡‡æ ·é—´éš”')
    parser.add_argument('--start', type=int, default=0, help='èµ·å§‹å¸§')
    parser.add_argument('--end', type=int, help='ç»“æŸå¸§')
    parser.add_argument('--output', help='è¾“å‡ºJSONæ–‡ä»¶')
    
    args = parser.parse_args()
    
    # æ‰§è¡Œæ£€æµ‹
    result = detect_video_faces(
        args.video,
        model=args.model,
        threshold=args.threshold,
        mode=args.mode,
        sample_interval=args.interval,
        start_frame=args.start,
        end_frame=args.end
    )
    
    # è¾“å‡ºç»Ÿè®¡
    print(f"\nâœ… æ£€æµ‹å®Œæˆ!")
    print(f"\nğŸ“Š ç»Ÿè®¡ç»“æœ:")
    print(f"  æ€»æ£€æµ‹æ•°: {result['statistics']['total_detections']}")
    print(f"  å”¯ä¸€äººæ•°: {result['statistics']['unique_persons']}")
    print(f"  åŒ…å«äººè„¸çš„å¸§æ•°: {result['statistics']['frames_with_faces']}")
    print(f"  å•å¸§æœ€å¤§äººè„¸æ•°: {result['statistics']['max_faces_per_frame']}")
    
    if result['persons']:
        print(f"\nğŸ‘¥ æ£€æµ‹åˆ°çš„äººå‘˜:")
        for person_id, info in result['persons'].items():
            print(f"  {person_id}:")
            if info['gender']:
                print(f"    æ€§åˆ«: {info['gender']}")
            if info['age']:
                print(f"    å¹´é¾„: {info['age']}")
            print(f"    å‡ºç°æ¬¡æ•°: {info['total_appearances']}")
            print(f"    æ—¶é—´èŒƒå›´: å¸§ {info['first_frame']}-{info['last_frame']}")
    
    # ä¿å­˜ç»“æœ
    if args.output:
        output_file = args.output
    else:
        output_file = Path(args.video).stem + '_faces.json'
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


if __name__ == '__main__':
    main()