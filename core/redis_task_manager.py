"""
åŸºäºRedisçš„ä»»åŠ¡ç®¡ç†å™¨
"""
import json
import time
import uuid
import random
from datetime import datetime
from typing import Dict, Any, Optional, List
from redis import Redis
from loguru import logger
from config.workflows import WORKFLOW_TEMPLATES
from config.environments import environment_manager


class RedisTaskManager:
    """åŸºäºRedisçš„åˆ†å¸ƒå¼ä»»åŠ¡ç®¡ç†å™¨"""

    def __init__(self, redis_client: Redis):
        self.redis = redis_client
        self.lock_timeout = 300  # ä»»åŠ¡é”è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        logger.info("ğŸš€ Redisä»»åŠ¡ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def create_task(self, workflow_name: str = None, environment: str = None,
                    task_data: Dict[str, Any] = None, source_channel: str = None,
                    params: Dict[str, Any] = None) -> Dict[str, Any]:
        """åˆ›å»ºæ–°ä»»åŠ¡"""
        task_id = f"task_{uuid.uuid4().hex[:8]}"

        # ç¡®å®šä»»åŠ¡çš„å·¥ä½œæµåç§°å’Œç›®æ ‡ç¯å¢ƒ
        if workflow_name:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ¢è„¸å·¥ä½œæµ
            if workflow_name == "face_swap" or workflow_name == "faceswap":
                # æ¢è„¸ä»»åŠ¡å¤„ç†
                environment_name = environment or "face_swap"
                target_port = 8000  # æ¢è„¸æœåŠ¡ç«¯å£

                # æ”¯æŒé€šè¿‡ params æˆ– task_data ä¼ é€’å‚æ•°
                if params and "input_data" in params:
                    task_data = params["input_data"]

                # éªŒè¯æ¢è„¸ä»»åŠ¡æ•°æ®
                if not task_data:
                    raise ValueError(
                        "Face swap tasks require task_data or params with input_data")

                required_fields = ["source_url", "target_url"]
                missing_fields = [field for field in required_fields
                                  if field not in task_data]
                if missing_fields:
                    raise ValueError(
                        f"Missing required fields: {missing_fields}")

                # ä½¿ç”¨ä¸ComfyUIç›¸åŒçš„params.input_data.wf_jsonæ ¼å¼
                task = {
                    "taskId": task_id,
                    "workflow": workflow_name,  # ä½¿ç”¨ä¸€è‡´çš„é”®å
                    "workflow_name": workflow_name,  # ä¿æŒå‘åå…¼å®¹
                    "environment": environment_name,
                    "target_port": target_port,
                    "params": {
                        "input_data": {
                            "wf_json": task_data  # ç»Ÿä¸€ä½¿ç”¨params.input_data.wf_json
                        }
                    },
                    "created_at": datetime.now().isoformat(),
                    "updated_at": datetime.now().isoformat(),
                    "status": "PENDING",
                    "source_channel": source_channel  # æ·»åŠ æºæ¸ é“ä¿¡æ¯
                }

                # ä¿å­˜åˆ°Redis
                self._save_task_to_redis(task)
                return task
            else:
                # éªŒè¯ComfyUIå·¥ä½œæµæ˜¯å¦å­˜åœ¨
                available_workflows = environment_manager.get_all_workflows()
                if workflow_name not in available_workflows:
                    raise ValueError(
                        f"æœªçŸ¥çš„å·¥ä½œæµ: {workflow_name}. å¯ç”¨å·¥ä½œæµ: {available_workflows}")
        else:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šå·¥ä½œæµï¼Œéšæœºé€‰æ‹©ä¸€ä¸ªå¯ç”¨çš„å·¥ä½œæµ
            available_workflows = environment_manager.get_all_workflows()
            workflow_name = random.choice(
                available_workflows) if available_workflows else "basic_generation"

        # ComfyUIä»»åŠ¡å¤„ç†
        # ä½¿ç”¨é»˜è®¤å·¥ä½œæµæ¨¡æ¿
        workflow = WORKFLOW_TEMPLATES["default"].copy()

        # éšæœºä¿®æ”¹å‚æ•°
        if "3" in workflow and workflow["3"]["class_type"] == "KSampler":
            workflow["3"]["inputs"]["seed"] = random.randint(1, 1000000)

        if "6" in workflow and workflow["6"]["class_type"] == "CLIPTextEncode":
            prompts = [
                "a beautiful landscape with mountains and rivers",
                "a cute cat sitting on a wooden table",
                "abstract art with vibrant colors and shapes",
                "a peaceful garden with blooming flowers",
                "a modern city skyline at golden hour"
            ]
            workflow["6"]["inputs"]["text"] = random.choice(prompts)

        # è·å–å·¥ä½œæµå¯¹åº”çš„ç¯å¢ƒä¿¡æ¯
        env_config = environment_manager.get_environment_by_workflow(
            workflow_name)
        environment_name = environment or (
            env_config.name if env_config else "comm")
        target_port = env_config.port if env_config else 3001

        task = {
            "taskId": task_id,
            "workflow": workflow_name,  # ä½¿ç”¨ä¸€è‡´çš„é”®å
            "workflow_name": workflow_name,  # ä¿æŒå‘åå…¼å®¹
            "environment": environment_name,
            "target_port": target_port,
            "params": {
                "input_data": {
                    "wf_json": workflow
                }
            },
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "status": "PENDING",
            "source_channel": source_channel  # æ·»åŠ æºæ¸ é“ä¿¡æ¯
        }

        # ä¿å­˜åˆ°Redis
        self._save_task_to_redis(task)
        return task

    def _save_task_to_redis(self, task: Dict[str, Any]):
        """ä¿å­˜ä»»åŠ¡åˆ°Redis"""
        task_id = task["taskId"]
        workflow_name = task.get("workflow") or task.get("workflow_name", "default")

        # ä½¿ç”¨Redis Pipelineæé«˜æ€§èƒ½
        pipe = self.redis.pipeline()

        # 1. ä¿å­˜ä»»åŠ¡è¯¦æƒ…åˆ°Hash
        task_key = f"task:{task_id}"
        pipe.hset(task_key, mapping={
            "taskId": task_id,
            "workflow": workflow_name,
            "workflow_name": workflow_name,
            "environment": task.get("environment", ""),
            "target_port": str(task.get("target_port", "")),
            "status": "PENDING",
            "params": json.dumps(task.get("params", {})),
            "created_at": task.get("created_at", ""),
            "updated_at": task.get("updated_at", ""),
            "source_channel": task.get("source_channel", "")
        })

        # 2. æ·»åŠ åˆ°å¾…å¤„ç†é˜Ÿåˆ—
        pipe.rpush("queue:pending", task_id)

        # å¦‚æœæœ‰ç‰¹å®šå·¥ä½œæµï¼Œä¹ŸåŠ åˆ°å·¥ä½œæµé˜Ÿåˆ—
        if workflow_name:
            pipe.rpush(f"queue:pending:{workflow_name}", task_id)

        # 3. æ·»åŠ åˆ°çŠ¶æ€ç´¢å¼•
        pipe.sadd("tasks:status:PENDING", task_id)

        # 4. æ·»åŠ åˆ°æ—¶é—´è½´
        timestamp = time.time()
        pipe.zadd("tasks:timeline", {task_id: timestamp})

        # 5. æ›´æ–°ç»Ÿè®¡
        pipe.hincrby("stats:global", "total_created", 1)
        if workflow_name:
            pipe.hincrby(f"stats:workflow:{workflow_name}", "created", 1)

        # æ‰§è¡Œæ‰€æœ‰æ“ä½œ
        pipe.execute()

        logger.info(f"âœ… ä»»åŠ¡åˆ›å»ºæˆåŠŸ: {task_id}, å·¥ä½œæµ: {workflow_name}")

    def get_next_task(self, workflow_names: Optional[List[str]] = None) -> Optional[Dict[str, Any]]:
        """è·å–ä¸‹ä¸€ä¸ªå¾…å¤„ç†ä»»åŠ¡ï¼ˆå¸¦åˆ†å¸ƒå¼é”ï¼‰

        Args:
            workflow_names: å¯é€‰çš„å·¥ä½œæµåç§°åˆ—è¡¨ï¼Œç”¨äºç­›é€‰ä»»åŠ¡
        """

        # ç¡®å®šè¦æŸ¥è¯¢çš„é˜Ÿåˆ—
        queues = []
        if workflow_names:
            # å¦‚æœæŒ‡å®šäº†å·¥ä½œæµç­›é€‰ï¼ŒæŸ¥æ‰¾åŒ¹é…çš„é˜Ÿåˆ—
            for wf in workflow_names:
                queues.append(f"queue:pending:{wf}")
            # ä¹Ÿæ£€æŸ¥é€šç”¨é˜Ÿåˆ—
            queues.append("queue:pending")
        else:
            # æ²¡æœ‰æŒ‡å®šç­›é€‰ï¼Œä½¿ç”¨é€šç”¨é˜Ÿåˆ—
            queues = ["queue:pending"]

        # è½®è¯¢æ‰€æœ‰é˜Ÿåˆ—
        for queue_key in queues:
            # æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦å­˜åœ¨
            if not self.redis.exists(queue_key):
                continue

            # ä»é˜Ÿåˆ—å·¦ä¾§å¼¹å‡ºä»»åŠ¡IDï¼ˆFIFOï¼‰
            task_id = self.redis.lpop(queue_key)

            if not task_id:
                continue

            task_id = task_id.decode() if isinstance(task_id, bytes) else task_id

            # å°è¯•è·å–ä»»åŠ¡é”
            lock_key = f"lock:task:{task_id}"
            lock = self.redis.set(lock_key, "locked", nx=True, ex=self.lock_timeout)

            if not lock:
                logger.warning(f"âš ï¸  ä»»åŠ¡ {task_id} å·²è¢«å…¶ä»–consumeré”å®šï¼Œè·³è¿‡")
                continue

            # è·å–ä»»åŠ¡è¯¦æƒ…
            task = self._get_task_by_id(task_id)

            if task:
                # å¦‚æœæŒ‡å®šäº†å·¥ä½œæµç­›é€‰ï¼ŒéªŒè¯ä»»åŠ¡å·¥ä½œæµ
                if workflow_names:
                    task_workflow = task.get("workflow_name") or task.get("workflow", "")
                    if task_workflow not in workflow_names:
                        # ä»»åŠ¡ä¸åŒ¹é…ï¼Œæ”¾å›é˜Ÿåˆ—å¹¶é‡Šæ”¾é”
                        logger.debug(f"ä»»åŠ¡ {task_id} å·¥ä½œæµä¸åŒ¹é…ï¼Œæ”¾å›é˜Ÿåˆ—")
                        self.redis.rpush(queue_key, task_id)
                        self.redis.delete(lock_key)
                        continue

                # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºFETCHED
                self._update_task_status_internal(
                    task_id,
                    "FETCHED",
                    old_status="PENDING"
                )

                logger.info(f"âœ… æˆåŠŸè·å–ä»»åŠ¡: {task_id}")
                return task

        # æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ä»»åŠ¡
        # å¦‚æœé˜Ÿåˆ—å¾ˆå°ï¼Œå°è¯•åˆ›å»ºæ–°ä»»åŠ¡
        if workflow_names and len(workflow_names) > 0:
            queue_size = self.redis.llen("queue:pending")
            if queue_size < 5:
                try:
                    workflow_name = random.choice(workflow_names)
                    new_task = self.create_task(workflow_name=workflow_name)
                    new_task["status"] = "FETCHED"
                    # æ›´æ–°çŠ¶æ€ä¸ºFETCHED
                    self._update_task_status_internal(
                        new_task["taskId"],
                        "FETCHED",
                        old_status="PENDING"
                    )
                    # è·å–é”
                    lock_key = f"lock:task:{new_task['taskId']}"
                    self.redis.set(lock_key, "locked", nx=True, ex=self.lock_timeout)
                    return new_task
                except Exception as e:
                    logger.debug(f"åˆ›å»ºæ–°ä»»åŠ¡å¤±è´¥: {e}")

        return None

    def update_task_status(self, task_id: str, status: str,
                           message: str = None, started_at: str = None,
                           finished_at: str = None, output_data: Dict = None) -> bool:
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        return self._update_task_status_internal(
            task_id, status, message=message, started_at=started_at,
            finished_at=finished_at, output_data=output_data
        )

    def _update_task_status_internal(self, task_id: str, status: str,
                                     old_status: str = None, message: str = None,
                                     started_at: str = None, finished_at: str = None,
                                     output_data: Dict = None) -> bool:
        """å†…éƒ¨çŠ¶æ€æ›´æ–°æ–¹æ³•"""
        task_key = f"task:{task_id}"

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
        if not self.redis.exists(task_key):
            logger.error(f"âŒ ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
            return False

        # è·å–æ—§çŠ¶æ€ï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if not old_status:
            old_status = self.redis.hget(task_key, "status")
            if old_status:
                old_status = old_status.decode() if isinstance(old_status, bytes) else old_status

        now = datetime.now().isoformat()

        # ä½¿ç”¨Pipelineæ‰¹é‡æ›´æ–°
        pipe = self.redis.pipeline()

        # 1. æ›´æ–°ä»»åŠ¡Hash
        updates = {
            "status": status,
            "updated_at": now
        }

        if message:
            updates["task_message"] = message
        if started_at:
            updates["started_at"] = started_at if isinstance(started_at, str) else started_at.isoformat()
        if finished_at:
            updates["finished_at"] = finished_at if isinstance(finished_at, str) else finished_at.isoformat()
        if output_data:
            updates["output_data"] = json.dumps(output_data)

        pipe.hset(task_key, mapping=updates)

        # 2. æ›´æ–°çŠ¶æ€ç´¢å¼•
        if old_status and old_status != status:
            pipe.srem(f"tasks:status:{old_status}", task_id)
        pipe.sadd(f"tasks:status:{status}", task_id)

        # 3. å¦‚æœæ˜¯å®ŒæˆçŠ¶æ€ï¼Œæ·»åŠ åˆ°å®Œæˆæ—¶é—´è½´
        if status in ["COMPLETED", "FAILED"]:
            timestamp = time.time()
            pipe.zadd(f"tasks:timeline:{status.lower()}", {task_id: timestamp})

            # æ›´æ–°ç»Ÿè®¡
            pipe.hincrby("stats:global", f"total_{status.lower()}", 1)

            # è·å–workflowå¹¶æ›´æ–°ç»Ÿè®¡
            workflow = self.redis.hget(task_key, "workflow")
            if workflow:
                workflow = workflow.decode() if isinstance(workflow, bytes) else workflow
                pipe.hincrby(f"stats:workflow:{workflow}", status.lower(), 1)

            # é‡Šæ”¾ä»»åŠ¡é”
            pipe.delete(f"lock:task:{task_id}")

        # æ‰§è¡Œæ‰€æœ‰æ“ä½œ
        pipe.execute()

        logger.debug(f"âœ… ä»»åŠ¡çŠ¶æ€æ›´æ–°: {task_id} {old_status} â†’ {status}")
        return True

    def _get_task_by_id(self, task_id: str) -> Optional[Dict[str, Any]]:
        """æ ¹æ®IDè·å–ä»»åŠ¡è¯¦æƒ…"""
        task_key = f"task:{task_id}"
        task_data = self.redis.hgetall(task_key)

        if not task_data:
            return None

        # è½¬æ¢bytesåˆ°å­—ç¬¦ä¸²
        task = {}
        for k, v in task_data.items():
            key = k.decode() if isinstance(k, bytes) else k
            value = v.decode() if isinstance(v, bytes) else v

            # è§£æJSONå­—æ®µ
            if key in ["params", "output_data"] and value:
                try:
                    task[key] = json.loads(value)
                except:
                    task[key] = value
            else:
                task[key] = value

        # ç¡®ä¿å‘åå…¼å®¹
        if "workflow" in task and "workflow_name" not in task:
            task["workflow_name"] = task["workflow"]

        return task

    def get_all_tasks(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰ä»»åŠ¡ï¼ˆåˆ†é¡µæ”¯æŒï¼‰"""
        # è·å–æ‰€æœ‰ä»»åŠ¡IDï¼ˆä»æ—¶é—´è½´ï¼‰
        task_ids = self.redis.zrevrange("tasks:timeline", 0, 99)  # æœ€æ–°100ä¸ª

        tasks = []
        for task_id in task_ids:
            task_id = task_id.decode() if isinstance(task_id, bytes) else task_id
            task = self._get_task_by_id(task_id)
            if task:
                tasks.append(task)

        # è·å–é˜Ÿåˆ—é•¿åº¦
        queue_length = self.redis.llen("queue:pending")

        return {
            "tasks": tasks,
            "queue_length": queue_length
        }

    def get_task_stats(self) -> Dict[str, Any]:
        """è·å–ä»»åŠ¡ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.redis.hgetall("stats:global")

        # è½¬æ¢bytes
        result = {}
        for k, v in stats.items():
            key = k.decode() if isinstance(k, bytes) else k
            try:
                value = int(v.decode() if isinstance(v, bytes) else v)
            except:
                value = 0
            result[key] = value

        # æ·»åŠ å®æ—¶çŠ¶æ€ç»Ÿè®¡
        result["pending"] = self.redis.scard("tasks:status:PENDING")
        result["fetched"] = self.redis.scard("tasks:status:FETCHED")
        result["processing"] = self.redis.scard("tasks:status:PROCESSING")
        result["completed"] = self.redis.scard("tasks:status:COMPLETED")
        result["failed"] = self.redis.scard("tasks:status:FAILED")

        return result

    def clear_all_tasks(self):
        """æ¸…ç©ºæ‰€æœ‰ä»»åŠ¡ï¼ˆå±é™©æ“ä½œï¼Œä»…ç”¨äºå¼€å‘/æµ‹è¯•ï¼‰"""
        # è·å–æ‰€æœ‰ä»»åŠ¡é”®
        task_keys = self.redis.keys("task:*")
        queue_keys = self.redis.keys("queue:*")
        stats_keys = self.redis.keys("stats:*")
        status_keys = self.redis.keys("tasks:status:*")
        timeline_keys = self.redis.keys("tasks:timeline*")
        lock_keys = self.redis.keys("lock:task:*")

        all_keys = task_keys + queue_keys + stats_keys + status_keys + timeline_keys + lock_keys

        if all_keys:
            self.redis.delete(*all_keys)

        logger.warning("âš ï¸  æ‰€æœ‰ä»»åŠ¡æ•°æ®å·²æ¸…ç©º")
