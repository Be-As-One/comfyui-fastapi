"""
ComfyUI ä»»åŠ¡å¤„ç†å™¨
"""
import json
import time
import httpx
import os
from datetime import datetime, timezone
from loguru import logger
from httpx_retries import RetryTransport, Retry
from consumer.processors.comfyui_api import ComfyUI, create_comfyui_client


class ComfyUIProcessor:
    """ComfyUIä»»åŠ¡å¤„ç†å™¨"""

    def __init__(self):
        # ä¸å†é¢„å…ˆåˆå§‹åŒ–å®¢æˆ·ç«¯ï¼Œè€Œæ˜¯æ ¹æ®ä»»åŠ¡åŠ¨æ€åˆ›å»º
        self.client_cache = {}  # ç¼“å­˜ä¸åŒå·¥ä½œæµçš„å®¢æˆ·ç«¯

        # åˆ›å»ºå¸¦é‡è¯•åŠŸèƒ½çš„HTTPå®¢æˆ·ç«¯
        retry = Retry(total=3, backoff_factor=0.5)
        self.retry_transport = RetryTransport(retry=retry)

    def _get_comfyui_client(self, task: dict) -> ComfyUI:
        """æ ¹æ®ä»»åŠ¡è·å–å¯¹åº”çš„ComfyUIå®¢æˆ·ç«¯"""
        # æ”¯æŒä¸¤ç§å‘½åæ–¹å¼ï¼šworkflowName (APIæ ¼å¼) å’Œ workflow_name (Pythonæ ¼å¼)
        workflow_name = task.get("workflowName") or task.get("workflow_name")

        logger.debug(f"è·å–ComfyUIå®¢æˆ·ç«¯: workflowName={task.get('workflowName')}, workflow_name={task.get('workflow_name')}, æœ€ç»ˆä½¿ç”¨: {workflow_name}")

        if workflow_name:
            # ä½¿ç”¨å·¥ä½œæµç‰¹å®šçš„å®¢æˆ·ç«¯
            if workflow_name not in self.client_cache:
                logger.info(f"ğŸ¯ åˆ›å»ºå·¥ä½œæµ '{workflow_name}' çš„ComfyUIå®¢æˆ·ç«¯")
                self.client_cache[workflow_name] = create_comfyui_client(
                    workflow_name=workflow_name)
            return self.client_cache[workflow_name]
        else:
            # ä½¿ç”¨é»˜è®¤å®¢æˆ·ç«¯
            if "default" not in self.client_cache:
                logger.info("ğŸ”§ åˆ›å»ºé»˜è®¤ComfyUIå®¢æˆ·ç«¯")
                comfyui_url = os.getenv('COMFYUI_URL', 'http://127.0.0.1:3002')
                if comfyui_url.startswith('http://'):
                    server_address = comfyui_url[7:]
                elif comfyui_url.startswith('https://'):
                    server_address = comfyui_url[8:]
                else:
                    server_address = comfyui_url
                self.client_cache["default"] = create_comfyui_client(
                    server_address=server_address)
            return self.client_cache["default"]

    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿è¿æ¥è¢«æ­£ç¡®å…³é—­"""
        try:
            logger.info("ğŸ”Œ å…³é—­æ‰€æœ‰ç¼“å­˜çš„ ComfyUI å®¢æˆ·ç«¯è¿æ¥")
            for workflow_name, client in self.client_cache.items():
                if client:
                    logger.debug(f"å…³é—­å®¢æˆ·ç«¯: {workflow_name}")
            self.client_cache.clear()
        except Exception as e:
            logger.error(f"å…³é—­ ComfyUI å®¢æˆ·ç«¯æ—¶å‡ºé”™: {e}")

    def process(self, task):
        """å¤„ç†ComfyUIä»»åŠ¡"""
        task_id = task.get("taskId")
        params = task.get("params", {})
        input_data = params.get("input_data", {})
        wf_json = input_data.get("wf_json", {})
        source_channel = task.get("source_channel")

        logger.info(f"å¼€å§‹å¤„ç†ComfyUIä»»åŠ¡: {task_id}")
        logger.debug(f"ä»»åŠ¡å‚æ•°éªŒè¯:")
        logger.debug(f"  - taskId: {task_id}")
        logger.debug(f"  - paramså­˜åœ¨: {bool(params)}")
        logger.debug(f"  - input_dataå­˜åœ¨: {bool(input_data)}")
        logger.debug(f"  - wf_jsonå­˜åœ¨: {bool(wf_json)}")
        logger.debug(f"  - source_channel: {source_channel}")

        if not task_id:
            logger.error("ä»»åŠ¡IDä¸ºç©ºï¼Œæ— æ³•å¤„ç†")
            return None

        task_started_at = datetime.now()

        if not wf_json:
            logger.error("å·¥ä½œæµJSONä¸ºç©ºï¼Œæ— æ³•å¤„ç†")
            self._update_task_status(task_id, "FAILED", message="å·¥ä½œæµJSONä¸ºç©º",
                                     started_at=task_started_at, source_channel=source_channel)
            return None

        logger.debug(
            f"å·¥ä½œæµJSONç»“æ„: {json.dumps(wf_json, indent=2, ensure_ascii=False)[:500]}...")

        try:
            # è®°å½•ä»»åŠ¡å¼€å§‹æ—¶é—´
            task_started_at = datetime.now(timezone.utc)

            # æ‰§è¡ŒComfyUIä»»åŠ¡å¤„ç†ï¼ˆåŒ…å«æ—©æœŸæœåŠ¡æ£€æŸ¥ï¼‰
            logger.info(
                f"ğŸ¯ å¼€å§‹æ‰§è¡ŒComfyUIå·¥ä½œæµ: {task_id} (å·¥ä½œæµ: {task.get('workflow_name', 'é»˜è®¤')})")
            t_gen_start = time.time()
            results = self._execute_comfyui_task(
                task, wf_json, task_id, task_started_at, source_channel)
            execution_time = time.time() - t_gen_start

            # æ£€æŸ¥æ˜¯å¦ä¸ºæœåŠ¡ä¸å¯ç”¨
            if results == "SERVICE_UNAVAILABLE":
                logger.info(f"ğŸ“‹ ä»»åŠ¡ {task_id} å› æœåŠ¡ä¸å¯ç”¨è¢«è·³è¿‡ï¼Œä¿æŒ PENDING çŠ¶æ€")
                return None  # è¿”å› Noneï¼Œä¸æ›´æ–°ä»»åŠ¡çŠ¶æ€

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºPROCESSINGï¼ˆåªæœ‰åœ¨æœåŠ¡å¯ç”¨æ—¶æ‰æ›´æ–°ï¼‰
            logger.debug(f"æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºPROCESSING: {task_id}")
            update_success = self._update_task_status(
                task_id, "PROCESSING", started_at=task_started_at, source_channel=source_channel)
            if not update_success:
                logger.warning(f"æ›´æ–°ä»»åŠ¡å¼€å§‹çŠ¶æ€å¤±è´¥ï¼Œä½†ç»§ç»­å¤„ç†: {task_id}")

            logger.info(f"å›¾åƒç”Ÿæˆè€—æ—¶: {execution_time:.2f} ç§’")
            logger.debug(f"ğŸ¯ ComfyUIæ‰§è¡Œå®Œæˆï¼Œå¼€å§‹åˆ†æç»“æœ:")
            logger.debug(f"  - resultsç±»å‹: {type(results)}")
            logger.debug(f"  - resultså€¼: {results}")
            logger.debug(f"  - resultsæ˜¯å¦ä¸ºNone: {results is None}")
            logger.debug(f"  - resultsæ˜¯å¦ä¸ºç©ºåˆ—è¡¨: {results == []}")
            if results:
                logger.debug(f"  - resultsé•¿åº¦: {len(results)}")
                for i, result in enumerate(results):
                    logger.debug(
                        f"  - result[{i}]: {result} (ç±»å‹: {type(result)})")

            # æ ¹æ®ç»“æœæ›´æ–°ä»»åŠ¡çŠ¶æ€
            # results ç°åœ¨æ˜¯ [{url, width, height, duration, ...}, ...] æ ¼å¼
            if results and len(results) > 0:
                logger.info(f"âœ… ä»»åŠ¡æ‰§è¡ŒæˆåŠŸï¼Œç”Ÿæˆäº† {len(results)} ä¸ªç»“æœ")

                # æå– URL åˆ—è¡¨ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
                urls = [r['url'] if isinstance(r, dict) else r for r in results]
                # å®Œæ•´ç»“æœåˆ—è¡¨ï¼ˆåŒ…å«å…ƒæ•°æ®ï¼‰
                media_results = results if isinstance(results[0], dict) else [{'url': r} for r in results]

                logger.debug(f"ğŸš€ å‡†å¤‡è°ƒç”¨_update_task_statusæ›´æ–°ä¸ºCOMPLETEDçŠ¶æ€")
                logger.debug(f"ğŸš€ output_data: urls={urls}, results={media_results}")

                update_success = self._update_task_status(
                    task_id, "COMPLETED",
                    output_data={
                        "urls": urls,
                        "results": media_results,  # åŒ…å« url, width, height, duration ç­‰
                    },
                    started_at=task_started_at,
                    finished_at=datetime.now(timezone.utc),
                    source_channel=source_channel
                )

                if update_success:
                    logger.info(f"âœ… ä»»åŠ¡å®ŒæˆçŠ¶æ€æ›´æ–°æˆåŠŸ: {task_id}")
                else:
                    logger.error(f"âŒ æ›´æ–°ä»»åŠ¡å®ŒæˆçŠ¶æ€å¤±è´¥: {task_id}")

                logger.debug(f"ğŸ¯ è¿”å›ç»“æœ: {results}")
                return results
            else:
                logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼šæ²¡æœ‰ç”Ÿæˆä»»ä½•ç»“æœ")
                logger.error(
                    f"âŒ è¯¦ç»†ä¿¡æ¯ - resultsç±»å‹: {type(results)}, resultså€¼: {results}")
                logger.debug(f"ğŸš€ å‡†å¤‡è°ƒç”¨_update_task_statusæ›´æ–°ä¸ºFAILEDçŠ¶æ€")

                update_success = self._update_task_status(
                    task_id, "FAILED",
                    message="No results generated.",
                    started_at=task_started_at,
                    finished_at=datetime.now(timezone.utc),
                    source_channel=source_channel
                )

                if update_success:
                    logger.info(f"âœ… ä»»åŠ¡å¤±è´¥çŠ¶æ€æ›´æ–°æˆåŠŸ: {task_id}")
                else:
                    logger.error(f"âŒ æ›´æ–°ä»»åŠ¡å¤±è´¥çŠ¶æ€å¤±è´¥: {task_id}")

                return None

        except Exception as e:
            logger.error(f"å¤„ç†ä»»åŠ¡æ—¶å‘ç”Ÿå¼‚å¸¸: {task_id}")
            logger.error(f"å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            logger.error(f"å¼‚å¸¸æ¶ˆæ¯: {str(e)}")
            logger.error(f"å¼‚å¸¸è¯¦æƒ…:", exc_info=True)

            try:
                update_success = self._update_task_status(
                    task_id, "FAILED",
                    message=str(e),
                    started_at=task_started_at,
                    finished_at=datetime.now(timezone.utc),
                    source_channel=source_channel
                )
                if not update_success:
                    logger.error(f"æ›´æ–°ä»»åŠ¡å¼‚å¸¸çŠ¶æ€å¤±è´¥: {task_id}")
            except Exception as update_error:
                logger.error(f"æ›´æ–°ä»»åŠ¡çŠ¶æ€æ—¶ä¹Ÿå‘ç”Ÿå¼‚å¸¸: {str(update_error)}")

            return None

    def _execute_comfyui_task(self, task, wf_json, task_id, task_started_at, source_channel=None):
        """æ‰§è¡ŒComfyUIä»»åŠ¡"""
        workflow_name = task.get("workflowName", "é»˜è®¤")
        environment = task.get("environment", "comm")
        target_port = task.get("target_port", 3001)

        logger.debug(f"ğŸ¯ å¼€å§‹æ‰§è¡ŒComfyUIå·¥ä½œæµ: {task_id}")
        logger.debug(f"  - å·¥ä½œæµ: {workflow_name}")
        logger.debug(f"  - ç¯å¢ƒ: {environment}")
        logger.debug(f"  - ç«¯å£: {target_port}")

        try:
            # æ ¹æ®ä»»åŠ¡è·å–å¯¹åº”çš„ComfyUIå®¢æˆ·ç«¯
            comfyui = self._get_comfyui_client(task)

            # æ—©æœŸæ£€æŸ¥ï¼šéªŒè¯ ComfyUI æœåŠ¡æ˜¯å¦å¯ç”¨
            logger.debug(f"æ£€æŸ¥ ComfyUI æœåŠ¡å¯ç”¨æ€§: {comfyui.server_address}")
            if not comfyui.check_server_health():
                logger.warning(
                    f"ComfyUI æœåŠ¡å™¨æ— å“åº”: {comfyui.server_address} (æ— æ³•è¿æ¥åˆ° /system_stats)")
                logger.info(f"è·³è¿‡ä»»åŠ¡ {task_id}ï¼Œç­‰å¾…æœåŠ¡å™¨å¯åŠ¨")
                return "SERVICE_UNAVAILABLE"  # è¿”å›ç‰¹æ®Šå€¼è¡¨ç¤ºæœåŠ¡ä¸å¯ç”¨

            logger.debug(f"âœ… ComfyUI æœåŠ¡å¯ç”¨ï¼Œç»§ç»­å¤„ç†ä»»åŠ¡")
            logger.debug(
                f"ğŸ”— ä½¿ç”¨ComfyUIå®¢æˆ·ç«¯ï¼Œè¿æ¥å¤ç”¨æ¬¡æ•°: {comfyui.connection_reuse_count}")

            logger.info(f"ğŸš€ å¼€å§‹ç”Ÿæˆå›¾åƒ (ç¯å¢ƒ: {environment}, ç«¯å£: {target_port})...")
            logger.debug(f"ğŸ¯ è°ƒç”¨comfyui.get_workflow_resultsï¼Œå‚æ•°:")
            logger.debug(f"  - wf_jsonç±»å‹: {type(wf_json)}")
            logger.debug(f"  - task_id: {task_id}")

            # é¢„å¤„ç†å·¥ä½œæµ
            wf_json = self._preprocess_workflow(wf_json)

            # åˆ›å»ºç®€å•çš„è¿›åº¦å›è°ƒå‡½æ•°
            def progress_callback(task_id, status, message):
                self._update_task_status(
                    task_id, status, message, started_at=task_started_at, source_channel=source_channel)

            # ä½¿ç”¨ get_workflow_results è·å–åŒ…å«å…ƒæ•°æ®çš„å®Œæ•´ç»“æœ
            # è¿”å›æ ¼å¼: [{url, width, height, duration?, format?}, ...]
            results = comfyui.get_workflow_results(
                wf_json, task_id, task_id=task_id, progress_callback=progress_callback)

            logger.debug(f"ğŸ¯ ComfyUI APIè¿”å›ç»“æœåˆ†æ:")
            logger.debug(f"  - resultsç±»å‹: {type(results)}")
            logger.debug(f"  - resultså€¼: {results}")
            logger.debug(f"  - resultsæ˜¯å¦ä¸ºNone: {results is None}")
            logger.debug(f"  - resultsæ˜¯å¦ä¸ºç©º: {not results}")
            if results:
                logger.debug(f"  - resultsé•¿åº¦: {len(results)}")
                logger.debug(f"  - resultså†…å®¹è¯¦ç»†:")
                for i, url in enumerate(results):
                    logger.debug(f"    [{i}]: {url} (ç±»å‹: {type(url)})")
            else:
                logger.debug(f"  - resultsä¸ºç©ºæˆ–Noneï¼Œæ— æ³•ç”Ÿæˆå›¾åƒ")

            logger.debug(f"ğŸ¯ _execute_comfyui_taskå³å°†è¿”å›: {results}")
            return results

        except ImportError as e:
            logger.error(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {str(e)}")
            raise
        except ConnectionRefusedError as e:
            # è¿æ¥è¢«æ‹’ç»ï¼Œè¯´æ˜æœåŠ¡æœªå¯åŠ¨
            logger.warning(f"ComfyUI è¿æ¥è¢«æ‹’ç»: {str(e)}")
            logger.info(f"è·³è¿‡ä»»åŠ¡ {task_id}ï¼Œç­‰å¾…æœåŠ¡å™¨å¯åŠ¨")
            return "SERVICE_UNAVAILABLE"
        except Exception as e:
            logger.error(f"æ‰§è¡ŒComfyUIä»»åŠ¡æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            logger.error(f"å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            logger.debug(f"å¼‚å¸¸è¯¦æƒ…:", exc_info=True)

            # æ£€æŸ¥æ˜¯å¦ä¸ºè¿æ¥ç›¸å…³é”™è¯¯
            error_msg = str(e).lower()
            if any(keyword in error_msg for keyword in ["connection", "websocket", "refused", "timeout", "not available"]):
                logger.warning(f"æ£€æµ‹åˆ°ç½‘ç»œè¿æ¥é”™è¯¯: {str(e)}")
                logger.info(f"è·³è¿‡ä»»åŠ¡ {task_id}ï¼Œç­‰å¾…è¿æ¥æ¢å¤")
                # æ¸…ç†å®¢æˆ·ç«¯ç¼“å­˜ï¼Œä¸‹æ¬¡é‡æ–°åˆ›å»º
                workflow_name = task.get("workflowName") or task.get("workflow_name")
                cache_key = workflow_name if workflow_name else "default"
                if cache_key in self.client_cache:
                    self.client_cache.pop(cache_key)
                return "SERVICE_UNAVAILABLE"

            # å…¶ä»–å¼‚å¸¸ç»§ç»­æŠ›å‡ºï¼Œå°†è¢«æ ‡è®°ä¸º FAILED
            raise

    def _preprocess_workflow(self, wf_json):
        """é¢„å¤„ç†å·¥ä½œæµ"""
        from services.media_service import media_service
        from services.node_service import node_service
        from services.lora_service import lora_service

        logger.debug(f"å¼€å§‹é¢„å¤„ç†å·¥ä½œæµ")
        logger.debug(f"å·¥ä½œæµåŒ…å« {len(wf_json)} ä¸ªèŠ‚ç‚¹")

        # 1. ä¿®å¤ Lora è·¯å¾„ï¼ˆå¤„ç†å­ç›®å½•é—®é¢˜ï¼‰
        try:
            wf_json = lora_service.fix_workflow_loras(wf_json)
        except Exception as e:
            logger.warning(f"Lora è·¯å¾„ä¿®å¤å¤±è´¥ï¼ˆç»§ç»­æ‰§è¡Œï¼‰: {e}")

        # ä½¿ç”¨èŠ‚ç‚¹å¤„ç†å™¨æ³¨å†Œè¡¨æ”¶é›†è¿œç¨‹URL
        logger.debug(f"å¼€å§‹æ”¶é›†è¿œç¨‹URL")
        remote_urls, url_to_node_mapping = node_service.collect_remote_urls(
            wf_json)
        logger.debug(f"æ”¶é›†åˆ° {len(remote_urls)} ä¸ªè¿œç¨‹URL")

        # å¦‚æœæœ‰è¿œç¨‹èµ„æºï¼Œä½¿ç”¨å¼‚æ­¥æ‰¹é‡ä¸‹è½½
        if remote_urls:
            logger.info(f"å¼€å§‹æ‰¹é‡ä¸‹è½½ {len(remote_urls)} ä¸ªè¿œç¨‹èµ„æº")
            logger.debug(f"è¿œç¨‹èµ„æºURLåˆ—è¡¨: {remote_urls}")
            logger.debug(f"URLåˆ°èŠ‚ç‚¹çš„æ˜ å°„å…³ç³»: {url_to_node_mapping}")

            try:
                # ä½¿ç”¨åª’ä½“æœåŠ¡æ‰¹é‡ä¸‹è½½ï¼ˆæ”¯æŒå›¾ç‰‡å’ŒéŸ³é¢‘ï¼‰
                logger.debug(
                    f"è°ƒç”¨ media_service.download_media_batch_sync å¼€å§‹ä¸‹è½½")
                download_results = media_service.download_media_batch_sync(
                    remote_urls)
                logger.debug(f"ä¸‹è½½å®Œæˆï¼Œç»“æœ: {download_results}")
                logger.info(f"æˆåŠŸä¸‹è½½ {len(download_results)} ä¸ªèµ„æº")

                # ä½¿ç”¨æ³¨å†Œè¡¨æ›´æ–°å·¥ä½œæµè·¯å¾„
                logger.debug(f"å¼€å§‹æ›´æ–°å·¥ä½œæµä¸­çš„è·¯å¾„")
                node_service.update_workflow_paths(
                    wf_json, download_results, url_to_node_mapping)
                logger.debug(f"å·¥ä½œæµè·¯å¾„æ›´æ–°å®Œæˆ")

                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹è½½å¤±è´¥çš„èµ„æº
                failed_urls = set(remote_urls) - set(download_results.keys())
                if failed_urls:
                    failed_urls_str = ', '.join(failed_urls)
                    logger.error(f"âŒ ä»¥ä¸‹èµ„æºä¸‹è½½å¤±è´¥: {failed_urls_str}")
                    raise Exception(f"é¢„å¤„ç†å·¥ä½œæµå¤±è´¥ï¼šæ— æ³•ä¸‹è½½èµ„æº {failed_urls_str}")
                else:
                    logger.debug(f"æ‰€æœ‰èµ„æºä¸‹è½½æˆåŠŸ")

            except Exception as e:
                logger.error(f"âŒ æ‰¹é‡ä¸‹è½½èµ„æºå¤±è´¥: {str(e)}")
                raise Exception(f"é¢„å¤„ç†å·¥ä½œæµå¤±è´¥ï¼š{str(e)}")

        logger.debug(f"é¢„å¤„ç†å®Œæˆ")
        return wf_json

    def _update_task_status(self, task_id, status, message=None,
                            started_at=None, finished_at=None, output_data=None, source_channel=None):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        from config.settings import task_api_url, TASK_CALLBACK_URL

        # è¯¦ç»†è°ƒè¯•ä¿¡æ¯
        logger.debug(f"_update_task_status: task_id={task_id}, status={status}")

        # ç¡®å®šå›è°ƒ URLï¼š
        # 1. source_channel æ˜¯æœ‰æ•ˆçš„ HTTP URL -> ä½¿ç”¨å®ƒ
        # 2. source_channel æ˜¯ "redis_queue" -> ä½¿ç”¨ TASK_CALLBACK_URL
        # 3. å¦åˆ™ä½¿ç”¨é»˜è®¤çš„ task_api_url
        logger.debug(f"å›è°ƒURLç¡®å®š: source_channel={source_channel}, TASK_CALLBACK_URL={TASK_CALLBACK_URL}, task_api_url={task_api_url}")

        if source_channel and source_channel.startswith(("http://", "https://")):
            update_url = source_channel
        elif source_channel == "redis_queue":
            if TASK_CALLBACK_URL:
                update_url = TASK_CALLBACK_URL
                logger.debug(f"Redisé˜Ÿåˆ—ä»»åŠ¡ï¼Œä½¿ç”¨ TASK_CALLBACK_URL: {update_url}")
            else:
                logger.warning(f"Redisé˜Ÿåˆ—ä»»åŠ¡ä½† TASK_CALLBACK_URL æœªé…ç½®ï¼Œè·³è¿‡å›è°ƒ: task_id={task_id}")
                return False
        else:
            update_url = task_api_url

        if not update_url:
            logger.warning(f"æ— å¯ç”¨çš„å›è°ƒURLï¼Œè·³è¿‡çŠ¶æ€æ›´æ–°: task_id={task_id}")
            return False

        url = f"{update_url}/api/comm/task/update"
        logger.debug(f"å›è°ƒURL: {url}")

        payload = {
            "taskId": task_id,
            "status": status,
            "started_at": started_at
        }

        if message:
            payload["task_message"] = message
        if started_at:
            payload["started_at"] = started_at.strftime(
                "%Y-%m-%d %H:%M:%S") if isinstance(started_at, datetime) else started_at
        if finished_at:
            payload["finished_at"] = finished_at.strftime(
                "%Y-%m-%d %H:%M:%S") if isinstance(finished_at, datetime) else finished_at
        if output_data:
            payload["output_data"] = output_data

        try:
            t_start = time.time()

            with httpx.Client(transport=self.retry_transport, timeout=30.0) as client:
                response = client.post(url, json=payload)
                response.raise_for_status()

                # å¤„ç†æ–°çš„ API å“åº”æ ¼å¼
                response_data = response.json()
                code = response_data.get("code")
                api_message = response_data.get("message", "")
                success = response_data.get("success", code == 200)

                if not success:
                    logger.error(f"å›è°ƒå¤±è´¥ task={task_id}: code={code}, message={api_message}")
                    return False

                logger.debug(f"å›è°ƒæˆåŠŸ task={task_id} status={status} è€—æ—¶{time.time() - t_start:.2f}s")
                return True

        except httpx.HTTPError as e:
            logger.error(f"å›è°ƒHTTPé”™è¯¯ task={task_id}: {str(e)}, url={url}")
            return False
        except Exception as e:
            logger.error(f"å›è°ƒå¼‚å¸¸ task={task_id}: {type(e).__name__}: {str(e)}")
            return False
