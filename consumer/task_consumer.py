"""
ä»»åŠ¡æ¶ˆè´¹è€…
"""
import asyncio
from typing import Optional
from utils import get_task_api_urls
import httpx
from loguru import logger
from consumer.processor_registry import processor_registry
from httpx_retries import RetryTransport, Retry
from utils.workflow_filter import workflow_filter


class TaskConsumer:
    """ä»»åŠ¡æ¶ˆè´¹è€…"""

    def __init__(self, name: str):
        self.name = name
        self.api_urls = get_task_api_urls()  # è·å–å¤šä¸ªAPI URL
        self.running = False
        self.processor_registry = processor_registry
        self.source_stats = {}
        # ä½¿ç”¨ httpx-retries åŒ…çš„é…ç½®
        retry = Retry(total=3, backoff_factor=0.5)
        self.retry_transport = RetryTransport(retry=retry)
        logger.info(f"ç»Ÿä¸€ä»»åŠ¡æ¶ˆè´¹è€… {self.name} åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"API URLs: {self.api_urls}")
        logger.info(
            f"æ”¯æŒçš„å¤„ç†å™¨: {list(self.processor_registry.list_processors().keys())}")

    async def fetch_task(self):
        """ä»å¤šä¸ªä»»åŠ¡æºè½®è¯¢è·å–ä¸€ä¸ªå¾…å¤„ç†ä»»åŠ¡"""
        allowed_workflows = workflow_filter.get_allowed_workflows()

        # ç¡®å®šè¦ä¼ é€’çš„å·¥ä½œæµç­›é€‰å‚æ•°
        workflow_names_param = None
        if allowed_workflows and '*' not in allowed_workflows:
            # æœ‰ç‰¹å®šçš„å…è®¸å·¥ä½œæµåˆ—è¡¨ï¼Œä½œä¸ºæ•°ç»„ä¼ é€’
            workflow_names_param = allowed_workflows
            logger.debug(f"ğŸ¯ ä½¿ç”¨å·¥ä½œæµç­›é€‰: {workflow_names_param}")
        else:
            logger.debug("ğŸ“‹ ä¸ä½¿ç”¨å·¥ä½œæµç­›é€‰ï¼ˆå…è®¸æ‰€æœ‰ï¼‰")

        # è½®è¯¢æ‰€æœ‰é…ç½®çš„APIæº
        for api_url in self.api_urls:
            url = f"{api_url}/api/comm/task/fetch"
            workflows_desc = ', '.join(workflow_names_param) if workflow_names_param else "any"
            logger.debug(f"Fetching task from: {url} (workflows: {workflows_desc})")

            task = await self._try_fetch_from_url(url, workflow_names_param)
            if task:
                logger.info(
                    f"âœ… Successfully got task {task.get('taskId')} from: {api_url} (workflows: {workflows_desc})")
                return task
            else:
                logger.debug(f"No task available from: {api_url} (workflows: {workflows_desc})")

        # æ‰€æœ‰æºéƒ½æ²¡æœ‰ä»»åŠ¡
        logger.debug("No tasks available from any configured source")
        return None

    async def _try_fetch_from_url(self, url: str, workflow_names: Optional[list] = None):
        """å°è¯•ä»å•ä¸ªURLè·å–ä»»åŠ¡

        Args:
            url: APIç«¯ç‚¹URL
            workflow_names: å¯é€‰çš„å·¥ä½œæµåç§°åˆ—è¡¨ï¼Œç”¨äºç­›é€‰ç‰¹å®šå·¥ä½œæµçš„ä»»åŠ¡
        """
        # è·å–APIåŸºç¡€URLï¼ˆç§»é™¤è·¯å¾„éƒ¨åˆ†ï¼‰
        api_base_url = url.split('/api/comm/task/fetch')[0]

        try:
            # æ„å»ºè¯·æ±‚å‚æ•°
            params = {}
            if workflow_names:
                # FastAPI çš„ Query(List[str]) éœ€è¦ä¼ é€’å¤šä¸ªåŒåå‚æ•°
                # httpx ä¼šè‡ªåŠ¨å¤„ç†åˆ—è¡¨å‚æ•°
                params['workflow_names'] = workflow_names

            async with httpx.AsyncClient(
                timeout=10.0,
                transport=self.retry_transport
            ) as client:
                response = await client.get(url, params=params)
                response.raise_for_status()

                response_data = response.json()

                if not isinstance(response_data, dict):
                    logger.debug(
                        f"APIè¿”å›äº†éå­—å…¸ç±»å‹çš„æ•°æ®: {type(response_data)} from {api_base_url}")
                    return None

                # å¤„ç†æ–°çš„ API å“åº”æ ¼å¼
                code = response_data.get("code")
                message = response_data.get("message", "")
                data = response_data.get("data")
                success = response_data.get("success", code == 200)

                if not success:
                    logger.debug(
                        f"APIè¯·æ±‚å¤±è´¥: code={code}, message={message} from {api_base_url}")
                    return None

                # ä» data å­—æ®µä¸­è·å–ä»»åŠ¡ä¿¡æ¯
                if not data:
                    logger.debug(
                        f"No task available (data is empty) from {api_base_url}")
                    return None

                task_id = data.get("taskId")
                if task_id:
                    logger.debug(f"Got task: {task_id} from {api_base_url}")
                    # ä¸ºä»»åŠ¡æ·»åŠ æºæ¸ é“ä¿¡æ¯
                    data["source_channel"] = api_base_url
                    logger.debug(
                        f"Task {task_id} marked with source_channel: {api_base_url}")
                    return data
                else:
                    logger.debug(f"No task available from {api_base_url}")
                    return None

        except httpx.HTTPError as e:
            logger.debug(
                f"Network error fetching task from {api_base_url}: {e}")
            return None
        except Exception as e:
            logger.debug(
                f"Unexpected error fetching task from {api_base_url}: {e}")
            return None

    async def process_task(self, task):
        """å¤„ç†å•ä¸ªä»»åŠ¡"""
        task_id = task.get("taskId")
        workflow_name = task.get("workflow_name", "")

        if not task_id:
            logger.error("Task missing taskId")
            return None

        # å…ˆæ£€æŸ¥å·¥ä½œæµæ˜¯å¦è¢«å…è®¸
        if not workflow_filter.is_workflow_allowed(workflow_name):
            logger.info(f"â­ï¸  è·³è¿‡ä»»åŠ¡ {task_id} - å·¥ä½œæµ '{workflow_name}' ä¸è¢«å½“å‰æœºå™¨å…è®¸")
            # è¿”å› Noneï¼Œä»»åŠ¡ä¿æŒ PENDING çŠ¶æ€ï¼Œè®©å…¶ä»–æœºå™¨å¤„ç†
            return None

        logger.info(f"å¼€å§‹å¤„ç†ä»»åŠ¡: {task_id} (å·¥ä½œæµ: {workflow_name})")
        logger.debug(f"ä»»åŠ¡è¯¦æƒ…: {task}")

        try:
            # æ ¹æ®å·¥ä½œæµåç§°è·å–å¯¹åº”çš„å¤„ç†å™¨
            processor = self.processor_registry.get_processor(workflow_name)

            if not processor:
                # è¿™ç§æƒ…å†µç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºå·²ç»åœ¨ä¸Šé¢æ£€æŸ¥è¿‡äº†
                logger.warning(f"âš ï¸  å·¥ä½œæµ '{workflow_name}' è¢«è¿‡æ»¤æˆ–æœªæ‰¾åˆ°å¤„ç†å™¨")
                return None

            processor_type = type(processor).__name__
            logger.info(f"ğŸ¯ ä½¿ç”¨å¤„ç†å™¨: {processor_type}")

            # ä½¿ç”¨å¤„ç†å™¨å¤„ç†ä»»åŠ¡ - åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥ä»£ç 
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(None, processor.process, task)

            if result:
                logger.info(f"âœ… ä»»åŠ¡ {task_id} å®Œæˆ (å¤„ç†å™¨: {processor_type})")
                logger.debug(f"ä»»åŠ¡ç»“æœ: {result}")
            else:
                logger.error(
                    f"âŒ ä»»åŠ¡ {task_id} å¤„ç†å¤±è´¥ - è¿”å›ç»“æœä¸ºç©º (å¤„ç†å™¨: {processor_type})")
                logger.error(f"ä»»åŠ¡è¯¦æƒ…: {task}")

            return result
        except Exception as e:
            logger.error(f"âŒ å¤„ç†ä»»åŠ¡ {task_id} æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            logger.error(f"å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            logger.error(f"ä»»åŠ¡è¯¦æƒ…: {task}")
            logger.debug(f"å¼‚å¸¸è¯¦æƒ…:", exc_info=True)
            return None

    async def start(self):
        """å¯åŠ¨æ¶ˆè´¹è€…å¾ªç¯"""
        self.running = True
        logger.info(f"ğŸš€ Consumer {self.name} å¯åŠ¨")

        while self.running:
            try:
                task = await self.fetch_task()
                if task:
                    await self.process_task(task)
                else:
                    await asyncio.sleep(1)
            except Exception as e:
                logger.error(f"Error in consumer loop: {e}")
                await asyncio.sleep(3)

    def stop(self):
        """åœæ­¢æ¶ˆè´¹è€…"""
        self.running = False
        logger.info(f"ğŸ›‘ Consumer {self.name} åœæ­¢")


async def start_consumer():
    """å¯åŠ¨consumerçš„å‡½æ•°ï¼Œä¾›main.pyè°ƒç”¨"""
    logger.info("ğŸš€ ç»Ÿä¸€ä»»åŠ¡æ¶ˆè´¹è€…å¯åŠ¨")
    logger.info("ğŸ¯ æ™ºèƒ½åˆ†å‘æ¨¡å¼ï¼šæ”¯æŒ ComfyUI å’Œ FaceFusion ä»»åŠ¡")

    # æ˜¾ç¤ºå½“å‰æœºå™¨çš„å·¥ä½œæµè¿‡æ»¤é…ç½®
    filter_stats = workflow_filter.get_filter_stats()
    logger.info("ğŸ”’ å·¥ä½œæµè¿‡æ»¤é…ç½®:")
    if filter_stats['allows_all']:
        logger.info("  - å…è®¸çš„å·¥ä½œæµ: æ‰€æœ‰")
    else:
        logger.info(
            f"  - å…è®¸çš„å·¥ä½œæµ: {', '.join(filter_stats['allowed_workflows'])}")

    # åˆ›å»ºç»Ÿä¸€çš„consumer
    consumer = TaskConsumer("unified-consumer")

    try:
        await consumer.start()
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œç³»ç»Ÿæ­£åœ¨å…³é—­")
    finally:
        consumer.stop()
        logger.info("âœ… ç»Ÿä¸€æ¶ˆè´¹è€…å·²åœæ­¢")
