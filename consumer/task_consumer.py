"""
ä»»åŠ¡æ¶ˆè´¹è€…
æ”¯æŒåŒæ¨¡å¼ï¼šHTTP è½®è¯¢ / Redis é˜Ÿåˆ—
"""
import asyncio
from utils import get_task_api_urls
import httpx
from loguru import logger
from consumer.processor_registry import processor_registry
from httpx_retries import RetryTransport, Retry
from utils.workflow_filter import workflow_filter
from config.settings import CONSUMER_MODE
from consumer.queue_consumer import get_queue_consumer
from consumer.task_schema import normalize_queue_task
from consumer.result_callback import get_result_callback


class TaskConsumer:
    """ä»»åŠ¡æ¶ˆè´¹è€… - æ”¯æŒ HTTP è½®è¯¢å’Œ Redis é˜Ÿåˆ—ä¸¤ç§æ¨¡å¼"""

    def __init__(self, name: str):
        self.name = name
        self.api_urls = get_task_api_urls()  # è·å–å¤šä¸ªAPI URL
        self.running = False
        self.processor_registry = processor_registry
        self.source_stats = {}
        self.consumer_mode = CONSUMER_MODE  # 'http' æˆ– 'redis_queue'
        self.queue_consumer = get_queue_consumer() if self.consumer_mode == 'redis_queue' else None
        self.result_callback = get_result_callback()

        # ä½¿ç”¨ httpx-retries åŒ…çš„é…ç½®
        retry = Retry(total=3, backoff_factor=0.5)
        self.retry_transport = RetryTransport(retry=retry)

        logger.info(f"ç»Ÿä¸€ä»»åŠ¡æ¶ˆè´¹è€… {self.name} åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"æ¶ˆè´¹æ¨¡å¼: {self.consumer_mode}")
        if self.consumer_mode == 'http':
            logger.info(f"API URLs: {self.api_urls}")
        elif self.consumer_mode == 'redis_queue':
            if self.queue_consumer and self.queue_consumer.is_available():
                logger.info("Redis é˜Ÿåˆ—æ¨¡å¼å·²å°±ç»ª")
            else:
                logger.warning("Redis é˜Ÿåˆ—ä¸å¯ç”¨ï¼Œå°†å›é€€åˆ° HTTP æ¨¡å¼")
                self.consumer_mode = 'http'
        logger.info(
            f"æ”¯æŒçš„å¤„ç†å™¨: {list(self.processor_registry.list_processors().keys())}")

    async def fetch_task(self):
        """ä»ä»»åŠ¡æºè·å–ä¸€ä¸ªå¾…å¤„ç†ä»»åŠ¡"""
        logger.debug(f'{self.consumer_mode}:fetch_task')
        # Redis é˜Ÿåˆ—æ¨¡å¼
        if self.consumer_mode == 'redis_queue' and self.queue_consumer:
            return await self._fetch_from_redis_queue()

        # HTTP è½®è¯¢æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
        return await self._fetch_from_http()

    async def _fetch_from_redis_queue(self):
        """ä» Redis ä¸‰çº§ä¼˜å…ˆé˜Ÿåˆ—è·å–ä»»åŠ¡"""
        try:
            raw_task = await self.queue_consumer.fetch_task()
            if raw_task:
                # æ ‡å‡†åŒ–ä»»åŠ¡æ ¼å¼
                task = normalize_queue_task(raw_task)

                # æ£€æŸ¥å·¥ä½œæµæ˜¯å¦è¢«å…è®¸ï¼ˆåœ¨æ‹‰å–åç«‹å³æ£€æŸ¥ï¼‰
                workflow_name = task.get('workflow') or task.get('workflowName', 'default')
                if not workflow_filter.is_workflow_allowed(workflow_name):
                    logger.warning(f"ğŸš« è·³è¿‡ä¸æ”¯æŒçš„å·¥ä½œæµ: {task.get('taskId')} ({workflow_name})")
                    # ä¸æ”¯æŒçš„ä»»åŠ¡ï¼Œæ”¾å›é˜Ÿåˆ—æˆ–æ ‡è®°å¤±è´¥
                    # è¿™é‡Œé€‰æ‹©ä¸è¿”å›ä»»åŠ¡ï¼Œè®©å®ƒç»§ç»­æ‹‰å–ä¸‹ä¸€ä¸ª
                    return None

                task["source_channel"] = "redis_queue"
                logger.info(f"ä» Redis é˜Ÿåˆ—è·å–ä»»åŠ¡: {task.get('taskId')}")
                return task
            return None
        except Exception as e:
            logger.error(f"ä» Redis é˜Ÿåˆ—è·å–ä»»åŠ¡å¤±è´¥: {e}")
            return None

    async def _fetch_from_http(self):
        """ä» HTTP API è½®è¯¢è·å–ä»»åŠ¡"""
        # è½®è¯¢æ‰€æœ‰é…ç½®çš„APIæº
        for api_url in self.api_urls:
            url = f"{api_url}/api/comm/task/fetch"
            logger.debug(f"Fetching task from: {url}")

            task = await self._try_fetch_from_url(url)
            if task:
                logger.info(
                    f"Successfully got task {task.get('taskId')} from: {api_url}")
                return task
            else:
                logger.debug(f"No task available from: {api_url}")

        # æ‰€æœ‰æºéƒ½æ²¡æœ‰ä»»åŠ¡
        logger.debug("No tasks available from any configured source")
        return None

    async def _try_fetch_from_url(self, url: str):
        """å°è¯•ä»å•ä¸ªURLè·å–ä»»åŠ¡"""
        # è·å–APIåŸºç¡€URLï¼ˆç§»é™¤è·¯å¾„éƒ¨åˆ†ï¼‰
        api_base_url = url.split('/api/comm/task/fetch')[0]

        try:
            # æ„å»ºè¯·æ±‚å‚æ•°ï¼Œæ·»åŠ å·¥ä½œæµç­›é€‰
            params = {}
            allowed_workflows = workflow_filter.get_allowed_workflows()

            # å¦‚æœæœ‰ç‰¹å®šçš„å…è®¸å·¥ä½œæµï¼ˆä¸æ˜¯å…è®¸æ‰€æœ‰ï¼‰ï¼Œåˆ™æ·»åŠ ç­›é€‰å‚æ•°
            if allowed_workflows and '*' not in allowed_workflows:
                params['workflowNames'] = allowed_workflows
                logger.debug(f"è¯·æ±‚ä»»åŠ¡æ—¶æ·»åŠ å·¥ä½œæµç­›é€‰: {allowed_workflows}")

            async with httpx.AsyncClient(
                timeout=10.0,
                transport=self.retry_transport
            ) as client:
                response = await client.get(url, params=params)
                response.raise_for_status()

                response_data = response.json()

                if not isinstance(response_data, dict):
                    logger.debug(
                        f"APIè¿”å›äº†éå­—å…¸ç±»å‹çš„æ•°æ®: {type(response_data)} from {api_base_url}")
                    return None

                # å¤„ç†æ–°çš„ API å“åº”æ ¼å¼
                code = response_data.get("code")
                message = response_data.get("message", "")
                data = response_data.get("data")
                success = response_data.get("success", code == 200)

                if not success:
                    logger.debug(
                        f"APIè¯·æ±‚å¤±è´¥: code={code}, message={message} from {api_base_url}")
                    return None

                # ä» data å­—æ®µä¸­è·å–ä»»åŠ¡ä¿¡æ¯
                if not data:
                    logger.debug(
                        f"No task available (data is empty) from {api_base_url}")
                    return None

                task_id = data.get("taskId")
                if task_id:
                    logger.debug(f"Got task: {task_id} from {api_base_url}")
                    # ä¸ºä»»åŠ¡æ·»åŠ æºæ¸ é“ä¿¡æ¯
                    data["source_channel"] = api_base_url
                    logger.debug(
                        f"Task {task_id} marked with source_channel: {api_base_url}")
                    return data
                else:
                    logger.debug(f"No task available from {api_base_url}")
                    return None

        except httpx.HTTPError as e:
            logger.debug(
                f"Network error fetching task from {api_base_url}: {e}")
            return None
        except Exception as e:
            logger.debug(
                f"Unexpected error fetching task from {api_base_url}: {e}")
            return None

    async def process_task(self, task):
        """å¤„ç†å•ä¸ªä»»åŠ¡"""
        task_id = task.get("taskId")
        # å…¼å®¹ä¸¤ç§å­—æ®µå: workflow_name å’Œ workflow
        workflow_name = task.get("workflow_name") or task.get("workflow", "")
        callback_url = task.get("callbackUrl") or task.get("callback_url")

        if not task_id:
            logger.error("Task missing taskId")
            return None

        # å…ˆæ£€æŸ¥å·¥ä½œæµæ˜¯å¦è¢«å…è®¸
        if not workflow_filter.is_workflow_allowed(workflow_name):
            logger.info(f"è·³è¿‡ä»»åŠ¡ {task_id} - å·¥ä½œæµ '{workflow_name}' ä¸è¢«å½“å‰æœºå™¨å…è®¸")
            # è¿”å› Noneï¼Œä»»åŠ¡ä¿æŒ PENDING çŠ¶æ€ï¼Œè®©å…¶ä»–æœºå™¨å¤„ç†
            return None

        logger.info(f"å¼€å§‹å¤„ç†ä»»åŠ¡: {task_id} (å·¥ä½œæµ: {workflow_name})")
        logger.debug(f"ä»»åŠ¡è¯¦æƒ…: {task}")

        # æ£€æµ‹æµ‹è¯•ä»»åŠ¡ï¼štaskId ä»¥ test_task_ å¼€å¤´ æˆ– workflowName æ˜¯ test_workflow
        is_test_task = (
            task_id.startswith("test_task_") or
            workflow_name == "test_workflow"
        )

        if is_test_task:
            logger.info(f"[TEST] æ£€æµ‹åˆ°æµ‹è¯•ä»»åŠ¡ {task_id}ï¼Œç›´æ¥æ ‡è®°å®Œæˆ")
            # æ„é€ æµ‹è¯•ç»“æœ
            test_result = {
                "status": "COMPLETED",
                "taskId": task_id,
                "message": "æµ‹è¯•ä»»åŠ¡å·²å®Œæˆï¼ˆè·³è¿‡å®é™…å¤„ç†ï¼‰",
                "is_test": True
            }
            # å‘é€æˆåŠŸå›è°ƒ
            if self.consumer_mode == 'redis_queue':
                await self.result_callback.send_success(
                    task_id=task_id,
                    result=test_result
                )
            logger.info(f"[TEST] æµ‹è¯•ä»»åŠ¡ {task_id} å·²æ ‡è®°å®Œæˆ")
            return test_result

        # Redis é˜Ÿåˆ—æ¨¡å¼ä¸‹æ ‡è®°ä»»åŠ¡ä¸ºå¤„ç†ä¸­
        if self.consumer_mode == 'redis_queue':
            await self.result_callback.send_processing(task_id)

        try:
            # æ ¹æ®å·¥ä½œæµåç§°è·å–å¯¹åº”çš„å¤„ç†å™¨
            processor = self.processor_registry.get_processor(workflow_name)

            if not processor:
                # è¿™ç§æƒ…å†µç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºå·²ç»åœ¨ä¸Šé¢æ£€æŸ¥è¿‡äº†
                logger.warning(f"å·¥ä½œæµ '{workflow_name}' è¢«è¿‡æ»¤æˆ–æœªæ‰¾åˆ°å¤„ç†å™¨")
                return None

            processor_type = type(processor).__name__
            logger.info(f"ä½¿ç”¨å¤„ç†å™¨: {processor_type}")

            # ä½¿ç”¨å¤„ç†å™¨å¤„ç†ä»»åŠ¡ - åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥ä»£ç 
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(None, processor.process, task)

            if result:
                logger.info(f"ä»»åŠ¡ {task_id} å®Œæˆ (å¤„ç†å™¨: {processor_type})")
                logger.debug(f"ä»»åŠ¡ç»“æœ: {result}")

                # Redis é˜Ÿåˆ—æ¨¡å¼ä¸‹å†™å…¥ç»“æœåˆ° Redis
                if self.consumer_mode == 'redis_queue':
                    await self.result_callback.send_success(
                        task_id=task_id,
                        result=result
                    )
            else:
                logger.error(
                    f"ä»»åŠ¡ {task_id} å¤„ç†å¤±è´¥ - è¿”å›ç»“æœä¸ºç©º (å¤„ç†å™¨: {processor_type})")
                logger.error(f"ä»»åŠ¡è¯¦æƒ…: {task}")

                # Redis é˜Ÿåˆ—æ¨¡å¼ä¸‹å†™å…¥å¤±è´¥ç»“æœåˆ° Redis
                if self.consumer_mode == 'redis_queue':
                    await self.result_callback.send_failure(
                        task_id=task_id,
                        error="å¤„ç†å™¨è¿”å›ç©ºç»“æœ"
                    )

            return result
        except Exception as e:
            logger.error(f"å¤„ç†ä»»åŠ¡ {task_id} æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            logger.error(f"å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            logger.error(f"ä»»åŠ¡è¯¦æƒ…: {task}")
            logger.debug(f"å¼‚å¸¸è¯¦æƒ…:", exc_info=True)

            # Redis é˜Ÿåˆ—æ¨¡å¼ä¸‹å†™å…¥å¼‚å¸¸ç»“æœåˆ° Redis
            if self.consumer_mode == 'redis_queue':
                await self.result_callback.send_failure(
                    task_id=task_id,
                    error=str(e)
                )

            return None

    async def start(self):
        """å¯åŠ¨æ¶ˆè´¹è€…å¾ªç¯"""
        self.running = True
        logger.info(f"Consumer {self.name} å¯åŠ¨")

        while self.running:
            try:
                task = await self.fetch_task()
                if task:
                    await self.process_task(task)
                else:
                    await asyncio.sleep(1)
            except Exception as e:
                logger.error(f"Error in consumer loop: {e}")
                await asyncio.sleep(3)

    def stop(self):
        """åœæ­¢æ¶ˆè´¹è€…"""
        self.running = False
        logger.info(f"Consumer {self.name} åœæ­¢")


async def start_consumer():
    """å¯åŠ¨consumerçš„å‡½æ•°ï¼Œä¾›main.pyè°ƒç”¨"""
    logger.info("ç»Ÿä¸€ä»»åŠ¡æ¶ˆè´¹è€…å¯åŠ¨")
    logger.info("æ™ºèƒ½åˆ†å‘æ¨¡å¼ï¼šæ”¯æŒ ComfyUI å’Œ FaceFusion ä»»åŠ¡")

    # æ˜¾ç¤ºå½“å‰æœºå™¨çš„å·¥ä½œæµè¿‡æ»¤é…ç½®
    filter_stats = workflow_filter.get_filter_stats()
    logger.info("å·¥ä½œæµè¿‡æ»¤é…ç½®:")
    if filter_stats['allows_all']:
        logger.info("  - å…è®¸çš„å·¥ä½œæµ: æ‰€æœ‰")
    else:
        logger.info(
            f"  - å…è®¸çš„å·¥ä½œæµ: {', '.join(filter_stats['allowed_workflows'])}")

    # åˆ›å»ºç»Ÿä¸€çš„consumer
    consumer = TaskConsumer("unified-consumer")

    try:
        await consumer.start()
    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œç³»ç»Ÿæ­£åœ¨å…³é—­")
    finally:
        consumer.stop()
        logger.info("ç»Ÿä¸€æ¶ˆè´¹è€…å·²åœæ­¢")
